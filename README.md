# aws-glue-libs
This repository contains libraries used in the [AWS Glue](https://aws.amazon.com/glue) service. These libraries extend [Apache Spark](https://spark.apache.org/) with additional data types and operations for ETL workflows. They are used in code generated by the AWS Glue service and can be used in scripts submitted with Glue jobs. 

## Content

- [awsglue](awsglue) -- This Python package includes the Python interfaces to the AWS Glue ETL library.

## Running gluepyspark shell, gluesparksubmit and pytest locally

The Glue ETL jars are now available via the maven build system in a s3 backed maven repository. We use the copy-dependencies target in
maven to get all the dependencies needed for glue locally.

Install apache maven from the following location:
https://aws-glue-etl-artifacts.s3.amazonaws.com/glue-common/apache-maven-3.6.0-bin.tar.gz

Install the spark distribution from the following location based on the glue version:
Glue version 0.9: https://aws-glue-etl-artifacts.s3.amazonaws.com/glue-0.9/spark-2.2.1-bin-hadoop2.7.tgz
Glue version 1.0: https://aws-glue-etl-artifacts.s3.amazonaws.com/glue-1.0/spark-2.4.3-bin-hadoop2.8.tgz

Export SPARK_HOME environment variable to extracted location of the
above spark archive.
Glue version 0.9: export SPARK_HOME=/home/$USER/spark-2.2.1-bin-hadoop2.7
Glue version 1.0: export SPARK_HOME=/home/$USER/spark-2.4.3-bin-spark-2.4.3-bin-hadoop2.8

The gluepytest script assumes that the pytest module is installed and available in the PATH

Glue shell: ./bin/gluepyspark
Glue submit: ./bin/gluesparksubmit
pytest: ./bin/gluepytest

## Licensing

The libraries in this repository licensed under the [Amazon Software License](http://aws.amazon.com/asl/) (the "License"). They may not be used except in compliance with the License, a copy of which is included here in the LICENSE file.

# Docker

The dockerized version allows to run Glue scripts locally or to connect to remote Glue Development endpoints.

Create the image with
```sh
docker build --squash --rm -f Dockerfile -t awsgluelibs:latest .
```

## Usage

First, export the AWS credentials, e.g. via the utility [aws-env](https://github.com/naftulikay/).

### Local PySpark

Run
```sh
docker run -it --rm \
    -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
    -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
    awsgluelibs:latest \
    gluepyspark
```
or
```sh
docker run -it --rm \
    -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
    -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
    awsgluelibs:latest \
    gluesparksubmit
```
to start a PySpark shell or submit a job locally.

### Local Zeppelin

Run
```sh
docker run -it --rm \
    -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
    -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
    awsgluelibs:latest \
    gluezeppelin
```
to use Apache Zeppelin with the local Spark instance. Point your browser at http://localhost:8080.

### Local Zeppelin with remote Glue development endpoint

Run
```sh
docker run -it --rm \
    -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
    -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
    awsgluelibs:latest \
    gluezeppelinremote
```
to use Apache Zeppelin with the remote Glue Development endpoint, then use SSH port forwarding to connect
```sh
ssh -i $PRIVATE_KEY_FILE_PATH -vNTL 9007:169.254.76.1:9007 glue@$DEV_ENDPOINT_PUBLIC_DNS
```

### Local Jupyter

Run
```sh
docker run -it --rm \
    -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
    -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
    awsgluelibs:latest \
    gluejupyter
```
to use Jupyter with the local Spark instance. Point your browser at http://localhost:8888 or use the URL displayed in the console.
